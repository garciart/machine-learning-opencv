<!doctype html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Test</title>
    <meta name="description" content="Test">
    <style>
        body {
            font-family: sans-serif;
            margin: 0 auto;
            width: 1024px;
            line-height: 1.5;
        }

        img {
            height: 480px;
        }

        pre {
            background-color: whitesmoke;
        }
    </style>
</head>

<body>
    <h1>Park!</h1>
    <div style="text-align: center;"><img src="README_images/park_demo.gif" alt="Welcome to Park!"
            style="width:100%;" /></div>
    <h2>Table of Contents</h2>
    <ul>
        <li><a href="#p1" title="Introduction">Introduction</a></li>
        <li><a href="#p2" title="Setting Up the Development Environment">Setting Up the Development Environment</a></li>
        <li><a href="#p3" title="Adding the Machine Learning Packages">Adding the Machine Learning Packages</a></li>
        <li><a href="#p4" title="Running the Scripts">Running the Scripts</a></li>
        <ul>
            <li><a href="#d1" title="OpenCV Image Capture and Display">OpenCV Image Capture and Display</a></li>
            <li><a href="#d2" title="Mask R-CNN Object Detection">Mask R-CNN Object Detection</a></li>
            <li><a href="#d3" title="Recognizing and Counting Vehicles">Recognizing and Counting Vehicles</a></li>
            <li><a href="#d4" title="Creating and Displaying zones">Creating and Displaying Zones</a></li>
            <li><a href="#d5" title="Counting Vehicles in Zones">Counting Vehicles in Zones</a></li>
            <li><a href="#d6" title="Creating the Database">Creating the Database</a></li>
            <li><a href="#d7" title="El Super Demo">El Super Demo</a></li>
        </ul>
        <li><a href="#p5" title="Schedule the Scripts Using cron">Schedule the Scripts Using cron</a></li>
        <li><a href="#p10" title="References">References</a>
    </ul>
    <br>
    <hr>
    <h2 id="p1">Introduction</h2>
    <p>As interns at NASA Langley, aka "The Sherpas", we worked on a lot of projects. Some of them involved machine
        learning, and, yes, this included a parking lot project.</p>
    <p>The use case was that finding parking is a problem for everyone. In 2016, drivers in New York City spent an
        average of 107 hours and $2243 a year looking for parking (INRIX, 2017). In addition, there are over 120
        outstanding requests for proposals for city and institutional parking management (IPMI, 2019). One of the
        original Sherpas, who had been working on a similar project at Georgia Tech, suggested creating a parking
        management system for NASA Langley's Digital Transformation initiative, and it was approved.</p>
    <p>We looked at various solutions, including sensors in each parking space; Light Detection and Ranging (LIDAR)
        to
        count cars entering and exiting parking areas; etc. Each approach had issues that made it impractical to
        implement with the resources we had; for example, most image recognition systems we looked at required a
        high,
        almost overhead, angle of view.</p>
    <p>Finally, we came accross <a
            href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400"
            target="_blank" title="Snagging parking spaces with Mask R-CNN and Python">Adam Geitgey's awesome
            article on
            object detection</a>, which introduced us to <a href="https://github.com/matterport/Mask_RCNN"
            target="_blank" title="Matterport Mask R-CNN">Matterport Mask R-CNN</a> (Geitgey, 2019). We realized
        that
        his implementation of Mask R-CNN was something we could use with the resources available to us, especially
        the
        lower and more practical viewing angle. However, Adam’s solution, designed for a single system for a single
        user
        looking at a single parking lot, was too expensive in terms of processing power and time. The first time we
        implemented it, it sent our cooling fans into overdrive and hung our computers for several minutes; run
        another
        of our projects, <a href="http://acronymsfortina.rgprogramming.com/index.html" target="_blank"
            title="Acronyms for Tina">Acronyms for Tina</a>, on localhost for a similar, albeit less intense,
        experience.</p>
    <p>Still, it worked, so using what we learned, we went back to the drawing board to develop a system that would
        rotate through multiple lots; analyze and collect parking data, and present this information to users in a
        timely manner. After much trial and error, we came up with a system that leveraged Linux scripting, Python,
        OpenCV, TensorFlow, SQL, and PHP to:</p>
    <ol>
        <li>Use a cron task scheduler to access the cameras overlooking each lot in turn.</li>
        <li>Capture a frame from each camera; divide it into zones (e.g., employee, handicap, etc.); and count the
            number of vehicles in each zone.</li>
        <li>Aggregate the results and collect them in a database.</li>
        <li>Have the front-end pull data for users from the database.</li>
    </ol>
    <p>Due to architecture, security requirements, cross-lot tracking, etc., the NASA application was a bit complex,
        and
        for those same reasons, we will not recreate it here. However, here are a series of demos, written in
        Python,
        that breakdown the way the machine learning aspect works. Besides a practical application of TensorFlow,
        Mask
        R-CNN, and computer vision, this is also a very good introduction to the Common Objects in COntext (COCO)
        dataset. Have fun and good luck!</p>
    <hr>
    <h2 id="p2">Setting Up the Development Environment</h2>
    <p>As we just said, we'll be using CentOS Linux 7 in VirtualBox for this demo, but you can use another virtual
        machine or an actual server if you like. Just make sure that your system has at least 2GB of memory; 16GB of
        hard disk space; 128MB of video memory; and a connection to the Internet.</p>
    <div style="background-color: lightgoldenrodyellow; padding: 6px;">
        <p><b>NOTE</b> - If you do choose to use CentOS or Red Hat Linux, we've included another README file name
            CENTOS with
            directions on how to set up your environment.</p>
    </div>
    <br>
    <div style="background-color: lightgoldenrodyellow; padding: 6px;">
        <p><b>NOTE</b> - While the production code for NASA only generated data, the demo scripts also display
            images. To
            view
            these images, you will need an X Server. For these demos, our solution was to use the GNOME Graphical
            User Interface
            (GUI)
            for development; it comes with a display server; it's lightweight; and it allows you to cut-and-paste
            from
            the host machine into the VM's Terminal. To use GNOME on CentOS or Red Hat Linux, use the following
            commands:</p>
        <pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
    </div>
    <p>Our first step is to make sure the operating system is up to date. On CentOS, we would use the following
        command;
        other flavors of Linux may use "sudo apt-get update" instead, while Windows users can run "wuauclt.exe
        /updatenow":</p>
    <pre>
[park@localhost ~]# sudo yum -y update
</pre>
    <p>This may take a while, especially on a new system.</p>
    <p>Once the system update is completed, make sure that the tools needed for development are installed:</p>
    <ol>
        <li><b>Python 3 Programming Language Interpreter and PIP Python Package Installer</b> - While Python 2 is
            installed with CentOS by default, we will need Python 3 to run our computer vision and machine learning
            scripts, specically Python 3.6.x. There are a few ways of doing this, but we will use the IUS Community
            Repo; for an in-depth look at options, check out <a href="https://www.hogarthuk.com/?q=node/15"
                title="Running newer applications on CentOS" target="_blank">this link from James Hogarth</a>. To
            install Python, run the following command:
            <pre>
[park@localhost ~]# sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm
[park@localhost ~]# sudo yum -y install python36u
[park@localhost ~]# sudo yum -y install python36u-pip
[park@localhost ~]# sudo yum -y install python36u-devel
[park@localhost ~]# python3 --version
[park@localhost ~]# pip3 --version
</pre>
        </li>
        <li><b>SQLite RDBMS</b> - For portability (and to keep this README as short as possible), we'll be using
            SQLite.
            Check the version using the following command:
            <pre>
[park@localhost ~]# sqlite3 -version
3.7.17 2013-05-20 00:56:22 118a3b35693b134d56ebd780123b7fd6f1497668
</pre>
            <p>If SQLite is not installed, install it using the following command:</p>
            <pre>
[park@localhost ~]# sudo yum -y install sqlite
</pre>
        </li>
        <li><b>cron Time-Based Job Scheduler</b> - cron should already be installed by default, but check anyway:
            <pre>
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
cronie-1.4.11-19.e17.x86_64
</pre>
            <p>If cron is not installed, install it using the following command:</p>
            <pre>[park@localhost ~]# yum -y install cronie</pre>
        </li>
    </ol>
    <p>Alright! Before continuing, let's do another update of the system using the following command:</p>
    <pre>[park@localhost ~]# sudo yum -y update</pre>
    <p>Just in case, we'll double check everything is installed and updated using the following commands:</p>
    <pre>
[park@localhost ~]# python3 ––version
[park@localhost ~]# pip3 ––version
[park@localhost ~]# sqlite3 -version
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
</pre>
    <figure style="text-align: center;"><img src="README_images/readme01.png" alt="Verifying initial setup" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Verifying initial setup
        </figcaption>
    </figure>
    </div>
    <hr>
    <h2 id="p3">Adding the Machine Learning Packages</h2>
    <p>Our next step is to add the packages we will need to run the scripts.</p>
    <div style="background-color: lightgoldenrodyellow; padding: 6px;">
        <p><b>NOTE</b> - If you like, you can clone this repository into your folder: <strong>Just make sure you
                do
                so
                before installing the ML packages.</strong> Use the following command to clone the repo:</p>
        <pre>git clone https://github.com/garciart/Park.git</pre>
        <p>This will create a folder "Park" with all the code in the right place. If you are using a shared
            folder,
            fetch into your shared folder instead of cloning:</p>
        <pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git init
[park@localhost Park]$ git remote add origin https://github.com/garciart/Park.git
[park@localhost Park]$ git fetch
[park@localhost Park]$ git checkout origin/master -ft
</pre>
        <p>As a rule, we don't use the "Master" branch, just like we don't use "root" for development. You don't
            have
            to, but we suggest creating a separate branch:</p>
        <pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git branch park # replace park with your username
[park@localhost Park]$ git checkout park # replace park with your username
[park@localhost Park]$ git status
</pre>
        <p>In addition, remember to use "Park" as your development folder, not the home folder (in our case,
            "park").
        </p>
    </div>
    <figure style="text-align: center;"><img src="README_images/readme02.png" alt="Cloning the Park repository" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Park repository
        </figcaption>
    </figure>
    </div>
    <p>Clone the excellent <a href="https://github.com/matterport/Mask_RCNN" target="_blank"
            title="Mask R-CNN engine from Matterport">Mask R-CNN engine from Matterport</a>, which will serve as
        our
        object detection and instance segmentation engine using the following commands:</p>
    <pre>
[park@localhost Park]$ git clone https://github.com/matterport/Mask_RCNN.git
[park@localhost Park]$ cd Mask_RCNN
[park@localhost Mask_RCNN]$ ls
[park@localhost Mask_RCNN]$ cat requirements.txt
</pre>
    <figure style="text-align: center;"><img src="README_images/readme03.png" alt="Cloning the Mask R-CNN repository" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Mask R-CNN
            repository
        </figcaption>
    </figure>
    </div>
    <p>Note the setup.py file, which will build and install the Mask RCNN engine, and the requirements.txt file,
        which
        contains the list of modules and packages we will need to run the Mask RCNN engine:</p>
    <ul>
        <li><a href="https://numpy.org/" title="numpy" target="_blank">numpy</a> - support for
            high-level mathematics involving arrays and matrices.</li>
        <li><a href="https://scipy.org/scipylib" title="scipy" target="_blank">scipy</a> -
            support for scientific and technical computing.</li>
        <li><a href="https://python-pillow.org/" title="Pillow" target="_blank">Pillow</a> -
            Python Imaging Library to open, manipulate and save image files.</li>
        <li><a href="https://cython.org/" title="cython" target="_blank">cython</a> - support for
            C extensions and datatypes in Python.</li>
        <li><a href="https://matplotlib.org/" title="matplotlib" target="_blank">matplotlib</a> -
            support for 2d plotting.</li>
        <li><a href="https://scikit-image.org/" title="scikit-image" target="_blank">scikit-image</a> - support
            for
            image processing.</li>
        <li><a href="https://www.tensorflow.org/" title="tensorflow" target="_blank">tensorflow>=1.3.0</a> -
            support
            for machine learning and tensor mathematics.</li>
        <li><a href="https://keras.io/" title="keras" target="_blank">keras>=2.0.8</a> - support
            for neural networks.</li>
        <li><a href="https://opencv.org/" title="opencv" target="_blank">opencv-python</a> -
            support for real-time computer vision.</li>
        <li><a href="https://www.h5py.org/" title="h5py" target="_blank">h5py</a> - support for
            implementing HDF5 binary data format.</li>
        <li><a href="https://imgaug.readthedocs.io/en/latest/" title="imgaug" target="_blank">imgaug</a> -
            support
            for for image augmentation in machine learning.</li>
        <li><a href="https://ipython.org/" title="IPython" target="_blank">IPython[all]</a> -
            support for interactive computing.</li>
    </ul>
    <p>However, before executing installing requirements.txt, we have to make some changes. Unfortunately,
        Matterport's
        version of Mask R-CNN cannot use TensorFlow 2.0 or Keras 2.3 or above. In addition, we will need to
        install
        <a href="https://docs.opencv.org/master/" title="opencv-python-contrib" target="_blank">OpenCV's extra
            modules</a> to display images. Using your favorite editor, change the TensorFlow and Keras lines to
        the
        following:</p>
    <pre>
tensorflow&gt;=1.3.0,&lt;2.0
keras&gt;=2.0.8,&lt;2.3
</pre>
    <p>In addition, add the following line after the OpenCV line:</p>
    <pre>
opencv-contrib-python
</pre>
    <figure style="text-align: center;"><img src="README_images/readme04.png" alt="Editing requirements.txt" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Editing requirements.txt
        </figcaption>
    </figure>
    </div>
    <p>We also have to make sure our <a href="https://setuptools.readthedocs.io/en/latest/" title="Setuptools"
            target="_blank">Setuptools</a> are up-to-date. Otherwise, we may run into errors
        extracting and creating modules and packages from requirements.txt. In addition, we have to make sure <a
            href="https://docs.python.org/3/library/tkinter.html" title="tkinter" target="_blank">tkinter, the
            standard Python interface to the Tk GUI toolkit</a>, is installed as well, or
        you may get a "matplotlib is currently using a non-GUI backend" error. Therefore, enter the following
        commands:
    </p>
    <pre>
[park@localhost Mask_RCNN]$ pip3 install --upgrade setuptools --user
[park@localhost Mask_RCNN]$ sudo yum -y install python3-tkinter
</pre>
    <p>Once the installation is complete, install Mask R-CNN's requirements using the following command:</p>
    <pre>
[park@localhost Mask_RCNN]$ pip3 install -r requirements.txt --user
</pre>
    <figure style="text-align: center;"><img src="README_images/readme05.png"
            alt="Installing Mask R-CNN requirements" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Installing Mask R-CNN
            requirements</figcaption>
    </figure>
    </div>
    <p>Hopefully, everything went well; if not, check the verbose comments for any errors and correct them.
        Remember, you may have to use a different flavor of a package depending on the system you are using.
        For
        example, we used Red Hat Linux, so for some packages, we had to use their version (e.g.,
        python36-mysql,
        etc.). You can look for packages using:</p>
    <pre>
[park@localhost Mask_RCNN]$ pip3 list
- or -
[park@localhost Mask_RCNN]$ yum search [package name]
</pre>
    <p>Once everything is set, run the setup script using the command:</p>
    <pre>
[park@localhost Mask_RCNN]$ python3 setup.py install --user
</pre>
    <figure style="text-align: center;"><img src="README_images/readme06.png" alt="Setting up Mask R-CNN" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Setting up Mask R-CNN
        </figcaption>
    </figure>
    </div>
    <p>Once again, check the verbose comments for any errors and correct them. Finally, go back one
        directory
        and download the Common Objects in Context (COCO) dataset by using the following commands:</p>
    <pre>
[park@localhost Mask_RCNN]$ cd ..
[park@localhost Park]$ wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
</pre>
    <p>The COCO dataset consists of over 100,000 images of 80 objects, including cars (#3), buses (#6),
        and
        trucks (#8); find out more at <a href="http://cocodataset.org" title="COCO Dataset"
            target="_blank">http://cocodataset.org</a>. Please note that you can replace
        this dataset with a more specific set for vehicle detection, but for now, we will use the COCO
        dataset:
    </p>
    <figure style="text-align: center;"><img src="README_images/readme07.png" alt="Getting the COCO dataset" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Getting the COCO
            dataset
        </figcaption>
    </figure>
    </div>
    <hr>
    <h2 id="p4">Running the Scripts</h2>
    <p>Now comes the fun part: Our Python code is a combination of Matterport's open-source Mask-RCNN's
        samples
        (Matterport, 2019), Alex Geitey's excellent article on detecting open parking spaces (Geitey,
        2019),
        and
        our own embellishments (all MIT licensed, of course). By the time you are done working through
        these
        demos, you should have a good understanding how the heavy lifting occurs on the back end.</p>
    <div style="background-color: lightgoldenrodyellow; padding: 6px;">
        <p><b>NOTE</b> - To view these images generated by the demo scripts, you will need an X Server;
            otherwise, OpenCV will display a "Failed to Start the X server" error. Our solution was to
            use
            the
            GNOME Graphical User Interface (GUI) for development; it came with a display server; it was
            lightweight; and it allowed us to cut-and-paste from the host machine into the VM's
            Terminal. To
            do
            so on Linux, use the following commands:</p>
        <pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
    </div>
    <p>Download the demos to your development folder if you did not clone or fetch the repo. There are
        seven
        demos, and each one adds new functionality to Park.</p>
    <ul>
        <li>demo1.py - OpenCV image capture and display demo.</li>
        <li>demo2.py - Adds Mask R-CNN object detection to the demo.</li>
        <li>demo3.py - Adds recognizing and counting vehicles to the demo.</li>
        <li>demo4.py - Adds creating and displaying zones to the demo.</li>
        <li>demo5.py - Adds counting vehicles in zones to the demo.</li>
        <li>demo6.py - Creates the database.</li>
        <li>demo7.py - El Super Demo: Incorporates communicating with the database and the
            functionality of
            all of the previous demos.</li>
    </ul>
    <p>You will also need the log, capture, image, and video folders. If you cloned or fetched the
        repository,
        your directory should look like this:</p>
    <figure style="text-align: center;"><img src="README_images/readme08.png" alt="Development folder listing" />
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Development folder
            listing
        </figcaption>
    </figure>
    </div>
    <p>Once you are all set up, open the code for each demo in your favorite editor as we get to it and
        follow
        along. By the way, each script has its own "shebang", so you can run them directly by setting
        their permissions to executable.</p>
    <hr>
    <h3 id="d1">Demo 1 - OpenCV Image Capture and Display</h3>
    <p>This one is pretty easy, but using OpenCV is the foundation of Park's image recognition system.
        Note
        that
        we listed four FRAME_SOURCE's (Lines 16-20), with three of them commented out:</p>
    <pre>
# Image, video or camera to process - set this to 0 to use your webcam instead of a file
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image1.jpg"),(IMAGE_DIR + "/demo_image2.jpg"),(IMAGE_DIR + "/demo_image3.jpg")]
# FRAME_SOURCE = [(VIDEO_DIR + "/demo_video1.mp4"),(VIDEO_DIR + "/demo_video2.mp4"),(VIDEO_DIR + "/demo_video3.mp4")]
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image.jpg")]
FRAME_SOURCE = ["https://raw.githubusercontent.com/garciart/Park/master/demo_images/demo_image.jpg"]
</pre>
    <p>Park can work with images, videos and live streams. While we use demo_image.jpg, feel free to
        switch
        sources during the first three demos. However, to work correctly with this tutorial, Demos 4
        through
        5
        need to use demo_image.jpg, while Demo 7 pulls what it needs from the database you will create
        in Demo 6.
    </p>
    <p>Note the colors. OpenCV uses a Blue-Green-Red (BGR) color model, which we converted to an RGB
        color
        model
        for Mask R-CNN (Line 34):</p>
    <pre>
# Convert the image from BGR color (which OpenCV uses) to RGB color
rgb_image = frame[:, :, ::-1]
</pre>
    <p>We did not notice a difference in detection accuracy when we ran the other scripts, but we do
        what the
        great guys at Matterport tell us
        to
        do!</p>
    <p>By the way, this script also saves a full-size copy of the image in the demo_captures folder,
        named "d1_capture.jpg". You will need this image for demo4.py to create zones using pixel
        coordinates.</p>
    <figure style="text-align: center;">
        <img src="README_images/readme09.png" alt="What Mask R-CNN sees">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">What Mask R-CNN
            sees</figcaption>
    </figure>
    <div style="background-color: lightgoldenrodyellow; padding: 6px;">
        <p><b>NOTE</b> - After some installations, you may receive the following error when attempting
            to
            use
            cv2.imshow():</p>
        <pre>
The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support.
</pre>
        <p>To correct this problem, force reinstall OpenCV using the following commands:</p>
        <pre>
pip3 install --upgrade --force-reinstall opencv-python --user
pip3 install --upgrade --force-reinstall opencv-contrib-python --user
</pre>
    </div>
    <hr>
    <h3 id="d2">Demo 2 - Mask R-CNN Object Detection</h3>
    <p>Speaking of Mask R-CNN, it is time to add it to the demo. As we stated earlier, we are using the
        Common
        Objects in Context (COCO) dataset for this demo. The COCO dataset has 80 classes of objects
        (lines
        46-63), of which we want three: cars (#3), trucks (#6), and buses (#8):</p>
    <pre>
# COCO Class names
# Index of the class in the list is its ID. For example, to get ID of
# the teddy bear class, use: class_names.index('teddy bear')
class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
                'bus', 'train', 'truck', 'boat', 'traffic light',
                'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',
                'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
                'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',
                'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
                'kite', 'baseball bat', 'baseball glove', 'skateboard',
                'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
                'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
                'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
                'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
                'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
                'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',
                'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',
                'teddy bear', 'hair drier', 'toothbrush']
</pre>
    <p>However, Mask R-CNN runs through all of the classes, making it inefficient. Eventually, we hope
        to
        replace it with a smaller dataset focused on vehicles, which will make the processing faster.
    </p>
    <p>The script loads a pre-trained model (line 44), collects the rgb_image from the camera, and then
        processes the image against the model, creating an array of detected objects (line 87):</p>
    <pre>
# Load pre-trained model
model.load_weights(COCO_MODEL_PATH, by_name=True)
...
...
# Run the image through the Mask R-CNN model to get results.
results = model.detect([rgb_image], verbose=0)
</pre>
    <p>We use the built-in visualize function to display the masks of these objects on the screen:</p>
    <pre>
# Show the frame of video on the screen
mrcnn.visualize.display_instances(rgb_image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])
</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme10.png" alt="Mask R-CNN masking and scoring">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Mask R-CNN masking
            and scoring</figcaption>
    </figure>
    <p>Once again, look at the code and note the changes between demo1.py and demo2.py, especially the
        imports. Also note that if the dataset you downloaded earlier is missing, the script will
        automatically
        download it
        from Matterport's GitHub repository, using mrcnn.utils on line 37. If you are behind a firewall,
        as we
        were at NASA, you can disabled this functionality with the following code:</p>
    <pre>
# Do not download COCO trained weights from Releases if needed!
if not os.path.exists(COCO_MODEL_PATH):
    # mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)
    print("Oh no! We are missing the mask_rcnn_coco.h5 dataset!")
</pre>
    <hr>
    <h3 id="d3">Demo 3 - Recognizing and Counting Vehicles</h3 id="d3">
    <p>In the previous demo, Mask R-CNN placed masks on every item it detected. However, as we stated
        earlier,
        we are only looking for vehicles. Therefore, we added the get_car_boxes function (line 25),
        which
        returns a numpy array of the cars (#3), trucks (#6), and buses (#8) that Mask R-CNN detected:
    </p>
    <pre>
# Filter a list of Mask R-CNN detection results to get only the detected cars / trucks
def get_car_boxes(boxes, class_ids):
    car_boxes = []
    for i, box in enumerate(boxes):
        # If the detected object isn't a car / truck, skip it
        if class_ids[i] in [3, 8, 6]:
            car_boxes.append(box)
    return np.array(car_boxes)
</pre>
    <p>We use that array with the OpenCV rectangle function to box the vehicles in (line 102):</p>
    <pre>
# Draw the box
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme11.png" alt="Identifying vehicles">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying
            vehicles</figcaption>
    </figure>
    <p>We also acquired their locations in the frame, which we will use this later to determine if a
        vehicle is
        within a zone.</p>
    <figure style="text-align: center;">
        <img src="README_images/readme11b.png" alt="Raw counts and coordinates">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Raw counts and
            coordinates</figcaption>
    </figure>
    <hr>
    <h3 id="d4">Demo 4 - Creating and Displaying Zones</h3 id="d4">
    <p>Next, we create the zones using OpenCV. We will only count cars within these zones, to avoid
        counting
        cars on roads, other parking lots, etc. In addition, each of these zones will have a
        zone type (e.g., employee, handicap, etc.), which will allow us to provide better counts to the
        users.</p>
    <p>Using the frame you captured in demo1.py and your favorite image viewer/editor, determine the
        pixel coordinates of each zone and enter them in the poly_coords array. For demo4.py, we created
        two zones, employee and handicap, and overlaid them on the frame:</p>
    <pre>
# Read clockwise from top-left corner
poly_coords = ([[816, 1150], [3200, 1140], [3200, 1350], [816, 1400]],
                [[240, 1140], [815, 1150], [815, 1400], [150, 1400]])

# BGR colors: Orange, Blue, Red, Gray, Yellow, Cyan, Pink, White
colors = [[0, 127, 255], [255, 0, 0], [0, 0, 255], [127, 127, 127],
            [0, 255, 255], [255, 255, 0], [127, 0, 255], [255, 255, 255]]

# Make an overlay for transparent boxes
overlay = frame.copy()

# Draw the filled zones
for index, p in enumerate(poly_coords, start=0):
    cv2.fillPoly(overlay, np.int32(
        [np.array(p)]), colors[index + 4])

# Set transparency for boxes
alpha = 0.4
# Add overlay to frame
frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)

# Optional Draw the zone boundaries
for index, p in enumerate(poly_coords, start=0):
    cv2.polylines(frame, np.int32(
        [np.array(p)]), True, colors[index], 10)

# Draw center crosshair
height, width, channels = frame.shape
cv2.drawMarker(frame, (int(width / 2), int(height / 2)),
                [255, 255, 0], cv2.MARKER_TRIANGLE_UP, 16, 2, cv2.LINE_4)
# Add timestamp
cv2.putText(frame, timestamp, (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX, 1, [0, 0, 255], 1)
</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme12.png" alt="Creating and displaying zones">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Creating and
            displaying zones</figcaption>
    </figure>
    <p>The cyan triangle in the middle is a calibration point. For
        Pan-Tilt-Zoom (PTZ) cameras, if the triangle and an outside reference point (e.g., a cone, sign,
        etc.) line up, the
        zone overlays are placed correctly on the frame. To see your work, this script also saves a
        time-stamped scaled copy of the image in the demo_captures folder, named
        "[YYMMDD]_d4_capture.jpg".</p>
    <hr>
    <h3 id="d5">Demo 5 - Counting Vehicles in Zones</h3>
    <p>Now that we have our zones, we will run Mask R-CNN again, but we will use Shapely's intersects
        function
        to to determine if a vehicle is in a zone (line 127). If a vehicle is in a zone, it is added to
        the
        zone's count (line 132). However, is is deleted from the numpy array, to avoid double counting
        cars that
        may intersect more than one zone (line 134):</p>
    <pre>
# Only show cars in the zones!
if((Polygon([(x1, y1), (x2, y1), (x1, y2), (x2, y2)])).intersects(Polygon(asPoint(array(p))))):
    # Draw the box and add to overlay
    cv2.rectangle(frame, (x1, y1), (x2, y2),
                    colors[index], 5)
    # Count car in zone
    count += 1
    # Delete the car to avoid double counting
    np.delete(car_boxes, box)
</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme13.png" alt="Identifying vehicles within zones">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying
            vehicles within zones</figcaption>
    </figure>
    <figure style="text-align: center;">
        <img src="README_images/readme14.png" alt="Counting vehicles within zones">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Counting vehicles
            within zones</figcaption>
    </figure>
    <hr>
    <h3 id="d6">Demo 6 - Creating the Database</h3>
    <p>Like we stated earlier, due to architecture, security requirements, cross-lot tracking, etc., the
        NASA application was complex, and required two separate databases. For our demo, we will use a
        simpler data model, and, for portabililty (and to keep this README from being longer than it
        already is!) we'll use SQLite to hold the data.</p>
    <p>As stated earlier, Park counts the number of vehicles in parking zones. We found that a single
        camera cannot always observe a complete lot. Instead, a camera can observe portions of multiple
        lots and a lot can be observed by multiple cameras. Therefore, we divided each feed into zones,
        with each zone associated with a lot, and, as shown in demo4.py, each zone is classified as
        employee parking, handicap parking, etc. In demo7.py. we will show you how Park cycles through
        and analyzes all the feeds every five minutes, collecting the counts for the zones that each
        camera can observe. Once the cycle is complete, Park can aggregate the counts into lot totals,
        which the front end can present to the user.</p>
    <p>Here's the data model:</p>
    <figure style="text-align: center;">
        <img src="README_images/readme15.png" alt="Park Data Model">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Park Data Model
        </figcaption>
    </figure>
    <p>And here are the table descriptions:</p>
    <ul>
        <li><b>Source:</b> This table contains a list of the sources, such as cameras, that collect images, as well as
            their location and credentials. The camera's URL is a unique value.</li>
        <li><b>Lot:</b> This table contains a list of parking lots, as well as their
            location in latitude and longitude. The lot name is a unique value.</li>
        <li><b>Type:</b> This table contains a list of zone types, such as employee parking,
            handicap parking, etc. The description is a unique value.</li>
        <li><b>Zone:</b> This table contains all the zones from all the lots in the database. It also contains the total
            number of parking spaces within each zone, as well as the boundaries of the zone within its source's frame.
            This table has a
            one-to-many relationship with Source, Lot, and Type, and a unique compound key
            comprised of the primary keys of each of those tables and its own ZoneID.</li>
        <li><b>OccupancyLog:</b> This table is a junction table (i.e., an associative entity) that
            collects and timestamps all the zone counts, providing both current and historical parking
            data. This table has a one-to-many relationship with Lot, Zone, and Type, and a
            unique compound key comprised of the primary keys of each of those tables and its own
            Timestamp.</li>
    </ul>
    <p>Run demo6.py to create the database in the db folder and open the database using the following commands:</p>
    <pre>
[park@localhost Park]$ ./demo6.py
[park@localhost Park]$ ls db
[park@localhost Park]$ sqlite3 db/park.db
</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme16.png" alt="Creating the Database">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Creating the Database
        </figcaption>
    </figure>
    <p>Once complete, test the new database with the some queries. Here are some examples; if they do not work, check
        for any errors that may have occurred when you created the database:</p>
    <pre>
SELECT * FROM Lot ORDER BY Name ASC;
SELECT * FROM Lot WHERE LotID = 1;
SELECT * FROM Type ORDER BY TypeID ASC;
SELECT * FROM Source ORDER BY SourceID ASC;
SELECT Zone.ZoneID, Source.Location, Type.Description, Lot.Name, Zone.TotalSpaces, Zone.PolyCoords
    FROM Zone
    INNER JOIN Source ON Zone.SourceID = Source.SourceID
    INNER JOIN Type ON Zone.TypeID = Type.TypeID
    INNER JOIN Lot ON Zone.LotID = Lot.LotID
    ORDER BY Lot.Name, Type.Description;</pre>
    <figure style="text-align: center;">
        <img src="README_images/readme17.png" alt="Running test queries">
        <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Running test queries
        </figcaption>
    </figure>
    <hr>
    <h3 id="d7">Demo 7 - Putting It All Together.</h3 id="d7">
        <figure style="text-align: center;">
            <img src="README_images/park_demo.gif" alt="Putting it all together">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Putting it
                all
                together</figcaption>
        </figure>
        <p>Finally! demo7.py is a super demo. It incorporates all of the functionality of the
            previous
            demos, and actually connects to the database to get feed credentials and to update data.</p>
        <div style="background-color: lightgoldenrodyellow; padding: 6px;">
            <p><strong>By the way, demo7.py requires user interaction after displaying each frame.
                    The
                    above image is an animation of the demo7.py results, created using GIMP.</strong>
            </p>
        </div>
    <hr>
    <div style="width: 100%; text-align: center;">
        <img src="sherpa_logo.png" alt="Sherpa Logo" style="height:240px;" />
    </div>
    <p style="text-align: center;">The Sherpas are Jrei Dimanno, Justine Forrest, Rob
        Garcia, John Graham,
        Tanner Griffin, Ryan Hashi, Brandon Hutton, Gabriel Jacobs, Fernando Jauregui,
        Wesley Madden, and
        Doug
        Trent</p>
    <hr>
    <h2 id="p10">References</h2>
    <p>Geitgey, A. (2019, January 21). Snagging parking spaces with Mask R-CNN and Python.
        Retrieved from <a
            href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400</p>"
            target="_blank"
            title="Snagging parking spaces with Mask R-CNN and Python">https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400
    </p></a>
    <p>INRIX. (2017, July 12). Searching for parking costs Americans $73 billion a year.
        Retrieved from <a href="http://inrix.com/press-releases/parking-pain-us/</p>" target="_blank"
            title="Searching for parking costs Americans $73 billion a year">http://inrix.com/press-releases/parking-pain-us/
    </p></a>
    <p>International Parking and Mobility Institute. (2015, October 12). Open requests for
        proposals.
        Retrieved
        July 31, 2019, from <a href="https://www.parking.org/membership/member-resources/rfps/</p>" target="_blank"
            title="Open requests for proposals">https://www.parking.org/membership/member-resources/rfps/
    </p>
    </a>
    <p>Matterport, Inc. (2019, March 10). matterport/Mask_RCNN. Retrieved August 2, 2019,
        from <a href="https://github.com/matterport/Mask_RCNN" target="_blank"
            title="matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a></p>
</body>

</html>